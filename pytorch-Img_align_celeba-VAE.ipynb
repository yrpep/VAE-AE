{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbyuFM9aj2UV","outputId":"28813153-d0b3-4e34-b785-d2058a22e5f5","executionInfo":{"status":"ok","timestamp":1684739222755,"user_tz":-480,"elapsed":81359,"user":{"displayName":"YL D","userId":"06733600836656624252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","320 80\n"]}],"source":["import os\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","from torchvision.utils import save_image\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","class CelebADataset(Dataset):\n","    def __init__(self, image_dir,  transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        #文件太大  选前400个\n","        self.file_list = os.listdir(image_dir)[:400]\n","    def __len__(self):\n","      return len(self.file_list)\n","    def __getitem__(self, idx):\n","        # 返回每个样本的图像和注释（如果有）\n","        image_path = os.path.join(self.image_dir, self.file_list[idx])\n","        image = Image.open(image_path).convert('RGB')\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        return image\n","\n","# 设置数据集路径\n","image_dir = '/content/drive/MyDrive/VAE/pytorch-mnist-VAE-master/img_align_celeba/img_align_celeba'\n","\n","# 定义数据预处理操作\n","transform = transforms.Compose([\n","    # 根据需要调整大小\n","    transforms.Resize((28, 28)),\n","    transforms.ToTensor(),\n","    # 根据需要进行归一化,\n","    #因为是自编码，这里可以选择保留原始方差和均值\n","    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","\n","# 创建CelebA数据集对象\n","celeba_dataset = CelebADataset(image_dir, transform)\n","\n","# 定义批量大小和是否打乱数据\n","batch_size = 64\n","shuffle = True\n","\n","# 创建数据加载器\n","celeba_loader = torch.utils.data.DataLoader(celeba_dataset, batch_size=batch_size, shuffle=shuffle)\n","train_ratio = 0.8  # 训练集所占比例\n","dataset_size = len(celeba_dataset)\n","train_size = int(train_ratio * dataset_size)\n","test_size = dataset_size - train_size\n","print(train_size,test_size)\n","train_dataset, test_dataset = random_split(celeba_dataset, [train_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"ScoM61r4l-Gc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ruk5H7wgoZhu","executionInfo":{"status":"ok","timestamp":1684551226182,"user_tz":-480,"elapsed":4,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"outputId":"4e2e0857-d627-41af-9cdb-df421e3baaca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QC1hqz2Gj2Ua","executionInfo":{"status":"ok","timestamp":1684735831450,"user_tz":-480,"elapsed":992,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4cd928f-5bed-43c4-d74c-e9e99aab8dc4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["VAE(\n","  (fc1): Linear(in_features=2352, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=256, bias=True)\n","  (fc31): Linear(in_features=256, out_features=2, bias=True)\n","  (fc32): Linear(in_features=256, out_features=2, bias=True)\n","  (fc4): Linear(in_features=2, out_features=256, bias=True)\n","  (fc5): Linear(in_features=256, out_features=512, bias=True)\n","  (fc6): Linear(in_features=512, out_features=2352, bias=True)\n",")"]},"metadata":{},"execution_count":13}],"source":["class VAE(nn.Module):\n","    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n","        super(VAE, self).__init__()\n","\n","        # encoder part\n","        self.fc1 = nn.Linear(x_dim, h_dim1)\n","        self.fc2 = nn.Linear(h_dim1, h_dim2)\n","        self.fc31 = nn.Linear(h_dim2, z_dim)\n","        self.fc32 = nn.Linear(h_dim2, z_dim)\n","        # decoder part\n","        self.fc4 = nn.Linear(z_dim, h_dim2)\n","        self.fc5 = nn.Linear(h_dim2, h_dim1)\n","        self.fc6 = nn.Linear(h_dim1, x_dim)\n","    #encoder: 编码器方法，接受一个输入x，经过两个全连接层和ReLU激活函数\n","    #output 两个向量mu和log_var，分别表示隐变量的均值和对数方差\n","    def encoder(self, x):\n","        h = F.relu(self.fc1(x))\n","        h = F.relu(self.fc2(h))\n","        return self.fc31(h), self.fc32(h) # mu, log_var\n","    #sampling: 采样方法，接受两个向量mu和log_var\n","    #根据公式z = mu + exp(0.5*log_var)*eps生成一个隐变量z\n","    #其中eps是一个标准正态分布的随机向量\n","    def sampling(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) # return z sample\n","    #decoder: 解码器方法，接受一个隐变量z，经过两个全连接层和ReLU激活函数\n","    #output 一个向量，经过sigmoid激活函数后表示重构的数据\n","    def decoder(self, z):\n","        h = F.relu(self.fc4(z))\n","        h = F.relu(self.fc5(h))\n","        return F.sigmoid(self.fc6(h))\n","    '''\n","    对给定图像数据，先调用encoder方法得到mu和log_var\n","    再调用sampling方法得到z\n","    再调用decoder方法得到重构的数据\n","    返回重构的数据，mu和log_var\n","    '''\n","    def forward(self, x):\n","        mu, log_var = self.encoder(x.view(-1,3* 784))\n","        z = self.sampling(mu, log_var)\n","        return self.decoder(z), mu, log_var\n","\n","# build model\n","vae = VAE(x_dim=3*784, h_dim1= 512, h_dim2=256, z_dim=2)\n","vae.to(device)"]},{"cell_type":"code","source":["for i in train_loader:\n","  print(i.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UvD5mG8xqwHZ","executionInfo":{"status":"ok","timestamp":1684477281673,"user_tz":-480,"elapsed":1517,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"outputId":"d392a24b-7875-4cd5-bda7-a3e28697c199"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 3, 28, 28])\n","torch.Size([64, 3, 28, 28])\n","torch.Size([64, 3, 28, 28])\n","torch.Size([64, 3, 28, 28])\n","torch.Size([64, 3, 28, 28])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSIV1Omnj2Ud"},"outputs":[],"source":["#VAE模型的所有参数做为adam优化器参数\n","optimizer = optim.Adam(vae.parameters())\n","# return reconstruction error + KL divergence losses\n","# 即二元交叉熵（BCE）来衡量重构误差，KL散度衡量隐变量分布和标准正态分布之间的差异\n","def loss_function(recon_x, x, mu, log_var):\n","    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 3*784), reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","    return BCE + KLD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLh7zQQOj2Ue"},"outputs":[],"source":["def train(epoch):\n","    #训练模式\n","    vae.train()\n","    #重置损失\n","    train_loss = 0\n","    for batch_idx, (data) in enumerate(train_loader):\n","        data = data.to(device)\n","        #重置梯度\n","        optimizer.zero_grad()\n","        #重构的数据，mu和log_var（隐变量的均值和对数方差）\n","        recon_batch, mu, log_var = vae(data)\n","\n","        #loss计算\n","\n","        loss = loss_function(recon_batch, data, mu, log_var)\n","\n","        loss.backward()\n","        train_loss += loss.item()\n","        #更新参数\n","        optimizer.step()\n","        #打印info\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89j8X1COj2Ue"},"outputs":[],"source":["def test():\n","    #评估模式\n","    vae.eval()\n","    #重置loss\n","    test_loss= 0\n","    with torch.no_grad():\n","      #正向传播，累计每个批次的损失值，取均值\n","        for data in test_loader:\n","            data = data.to(device)\n","            recon, mu, log_var = vae(data)\n","\n","            # sum up batch loss\n","            test_loss += loss_function(recon, data, mu, log_var).item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('====> Test set loss: {:.4f}'.format(test_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"0N9X0r4bj2Uf","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1684551692659,"user_tz":-480,"elapsed":444134,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"outputId":"2b82c2d9-d66c-405f-e2f6-713826147ad7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/320 (0%)]\tLoss: 1635.452515\n","====> Epoch: 1 Average loss: 1615.5004\n","====> Test set loss: 1590.9292\n","Train Epoch: 2 [0/320 (0%)]\tLoss: 1574.061646\n","====> Epoch: 2 Average loss: 1576.3232\n","====> Test set loss: 1567.9051\n","Train Epoch: 3 [0/320 (0%)]\tLoss: 1554.527588\n","====> Epoch: 3 Average loss: 1555.3573\n","====> Test set loss: 1543.0051\n","Train Epoch: 4 [0/320 (0%)]\tLoss: 1537.536499\n","====> Epoch: 4 Average loss: 1523.8096\n","====> Test set loss: 1512.3978\n","Train Epoch: 5 [0/320 (0%)]\tLoss: 1518.801636\n","====> Epoch: 5 Average loss: 1493.5364\n","====> Test set loss: 1481.6777\n","Train Epoch: 6 [0/320 (0%)]\tLoss: 1496.165283\n","====> Epoch: 6 Average loss: 1460.4941\n","====> Test set loss: 1466.5116\n","Train Epoch: 7 [0/320 (0%)]\tLoss: 1455.150146\n","====> Epoch: 7 Average loss: 1448.4058\n","====> Test set loss: 1456.5253\n","Train Epoch: 8 [0/320 (0%)]\tLoss: 1458.626953\n","====> Epoch: 8 Average loss: 1439.0099\n","====> Test set loss: 1470.5283\n","Train Epoch: 9 [0/320 (0%)]\tLoss: 1429.640137\n","====> Epoch: 9 Average loss: 1431.3832\n","====> Test set loss: 1454.2132\n","Train Epoch: 10 [0/320 (0%)]\tLoss: 1422.646851\n","====> Epoch: 10 Average loss: 1419.3416\n","====> Test set loss: 1437.3203\n","Train Epoch: 11 [0/320 (0%)]\tLoss: 1403.137451\n","====> Epoch: 11 Average loss: 1414.0177\n","====> Test set loss: 1441.9769\n","Train Epoch: 12 [0/320 (0%)]\tLoss: 1393.713501\n","====> Epoch: 12 Average loss: 1412.2003\n","====> Test set loss: 1433.1761\n","Train Epoch: 13 [0/320 (0%)]\tLoss: 1425.270264\n","====> Epoch: 13 Average loss: 1405.7690\n","====> Test set loss: 1424.7568\n","Train Epoch: 14 [0/320 (0%)]\tLoss: 1388.122070\n","====> Epoch: 14 Average loss: 1400.1804\n","====> Test set loss: 1424.7262\n","Train Epoch: 15 [0/320 (0%)]\tLoss: 1374.471558\n","====> Epoch: 15 Average loss: 1399.6395\n","====> Test set loss: 1422.4720\n","Train Epoch: 16 [0/320 (0%)]\tLoss: 1427.130005\n","====> Epoch: 16 Average loss: 1396.9056\n","====> Test set loss: 1424.2761\n","Train Epoch: 17 [0/320 (0%)]\tLoss: 1392.601929\n","====> Epoch: 17 Average loss: 1397.9697\n","====> Test set loss: 1419.7519\n","Train Epoch: 18 [0/320 (0%)]\tLoss: 1381.277222\n","====> Epoch: 18 Average loss: 1395.1169\n","====> Test set loss: 1421.4156\n","Train Epoch: 19 [0/320 (0%)]\tLoss: 1398.190063\n","====> Epoch: 19 Average loss: 1394.3144\n","====> Test set loss: 1418.8941\n","Train Epoch: 20 [0/320 (0%)]\tLoss: 1406.275391\n","====> Epoch: 20 Average loss: 1392.8964\n","====> Test set loss: 1418.7285\n","Train Epoch: 21 [0/320 (0%)]\tLoss: 1398.367310\n","====> Epoch: 21 Average loss: 1391.0745\n","====> Test set loss: 1418.5913\n","Train Epoch: 22 [0/320 (0%)]\tLoss: 1424.732300\n","====> Epoch: 22 Average loss: 1390.3263\n","====> Test set loss: 1415.9839\n","Train Epoch: 23 [0/320 (0%)]\tLoss: 1387.511108\n","====> Epoch: 23 Average loss: 1390.2880\n","====> Test set loss: 1417.4400\n","Train Epoch: 24 [0/320 (0%)]\tLoss: 1396.152222\n","====> Epoch: 24 Average loss: 1389.2246\n","====> Test set loss: 1416.7778\n","Train Epoch: 25 [0/320 (0%)]\tLoss: 1416.319946\n","====> Epoch: 25 Average loss: 1389.0101\n","====> Test set loss: 1416.8828\n","Train Epoch: 26 [0/320 (0%)]\tLoss: 1343.538452\n","====> Epoch: 26 Average loss: 1389.5483\n","====> Test set loss: 1416.0838\n","Train Epoch: 27 [0/320 (0%)]\tLoss: 1403.720459\n","====> Epoch: 27 Average loss: 1388.8223\n","====> Test set loss: 1416.3159\n","Train Epoch: 28 [0/320 (0%)]\tLoss: 1416.607300\n","====> Epoch: 28 Average loss: 1388.1092\n","====> Test set loss: 1416.1778\n","Train Epoch: 29 [0/320 (0%)]\tLoss: 1409.859741\n","====> Epoch: 29 Average loss: 1387.8802\n","====> Test set loss: 1416.1462\n","Train Epoch: 30 [0/320 (0%)]\tLoss: 1417.380005\n","====> Epoch: 30 Average loss: 1387.2825\n","====> Test set loss: 1416.4213\n","Train Epoch: 31 [0/320 (0%)]\tLoss: 1385.388916\n","====> Epoch: 31 Average loss: 1387.2084\n","====> Test set loss: 1416.9872\n","Train Epoch: 32 [0/320 (0%)]\tLoss: 1398.038940\n","====> Epoch: 32 Average loss: 1386.5552\n","====> Test set loss: 1415.7606\n","Train Epoch: 33 [0/320 (0%)]\tLoss: 1377.719482\n","====> Epoch: 33 Average loss: 1387.0731\n","====> Test set loss: 1418.6242\n","Train Epoch: 34 [0/320 (0%)]\tLoss: 1383.133667\n","====> Epoch: 34 Average loss: 1386.8603\n","====> Test set loss: 1415.6342\n","Train Epoch: 35 [0/320 (0%)]\tLoss: 1414.660522\n","====> Epoch: 35 Average loss: 1386.9954\n","====> Test set loss: 1416.9569\n","Train Epoch: 36 [0/320 (0%)]\tLoss: 1378.236328\n","====> Epoch: 36 Average loss: 1388.6885\n","====> Test set loss: 1421.0080\n","Train Epoch: 37 [0/320 (0%)]\tLoss: 1380.703247\n","====> Epoch: 37 Average loss: 1387.6267\n","====> Test set loss: 1415.7726\n","Train Epoch: 38 [0/320 (0%)]\tLoss: 1365.535156\n","====> Epoch: 38 Average loss: 1386.2785\n","====> Test set loss: 1417.6527\n","Train Epoch: 39 [0/320 (0%)]\tLoss: 1389.840576\n","====> Epoch: 39 Average loss: 1387.1503\n","====> Test set loss: 1417.1403\n","Train Epoch: 40 [0/320 (0%)]\tLoss: 1340.027588\n","====> Epoch: 40 Average loss: 1385.6022\n","====> Test set loss: 1414.9212\n","Train Epoch: 41 [0/320 (0%)]\tLoss: 1414.884033\n","====> Epoch: 41 Average loss: 1384.6295\n","====> Test set loss: 1416.9470\n","Train Epoch: 42 [0/320 (0%)]\tLoss: 1389.432373\n","====> Epoch: 42 Average loss: 1384.6213\n","====> Test set loss: 1416.8984\n","Train Epoch: 43 [0/320 (0%)]\tLoss: 1396.432739\n","====> Epoch: 43 Average loss: 1383.8993\n","====> Test set loss: 1415.7538\n","Train Epoch: 44 [0/320 (0%)]\tLoss: 1376.732178\n","====> Epoch: 44 Average loss: 1383.7336\n","====> Test set loss: 1415.5540\n","Train Epoch: 45 [0/320 (0%)]\tLoss: 1395.913696\n","====> Epoch: 45 Average loss: 1383.4514\n","====> Test set loss: 1415.7202\n","Train Epoch: 46 [0/320 (0%)]\tLoss: 1381.895996\n","====> Epoch: 46 Average loss: 1383.2043\n","====> Test set loss: 1416.3330\n","Train Epoch: 47 [0/320 (0%)]\tLoss: 1405.852295\n","====> Epoch: 47 Average loss: 1383.8508\n","====> Test set loss: 1415.7405\n","Train Epoch: 48 [0/320 (0%)]\tLoss: 1376.122070\n","====> Epoch: 48 Average loss: 1383.7272\n","====> Test set loss: 1415.7977\n","Train Epoch: 49 [0/320 (0%)]\tLoss: 1391.482300\n","====> Epoch: 49 Average loss: 1384.0816\n","====> Test set loss: 1416.0096\n","Train Epoch: 50 [0/320 (0%)]\tLoss: 1395.966431\n","====> Epoch: 50 Average loss: 1382.4283\n","====> Test set loss: 1416.5263\n","Train Epoch: 51 [0/320 (0%)]\tLoss: 1380.127441\n","====> Epoch: 51 Average loss: 1382.2457\n","====> Test set loss: 1415.0274\n","Train Epoch: 52 [0/320 (0%)]\tLoss: 1347.503296\n","====> Epoch: 52 Average loss: 1382.3049\n","====> Test set loss: 1417.1993\n","Train Epoch: 53 [0/320 (0%)]\tLoss: 1368.733276\n","====> Epoch: 53 Average loss: 1382.4679\n","====> Test set loss: 1415.3793\n","Train Epoch: 54 [0/320 (0%)]\tLoss: 1350.805420\n","====> Epoch: 54 Average loss: 1381.9322\n","====> Test set loss: 1415.2053\n","Train Epoch: 55 [0/320 (0%)]\tLoss: 1333.832275\n","====> Epoch: 55 Average loss: 1381.2157\n","====> Test set loss: 1417.0558\n","Train Epoch: 56 [0/320 (0%)]\tLoss: 1399.142212\n","====> Epoch: 56 Average loss: 1380.5714\n","====> Test set loss: 1416.0042\n","Train Epoch: 57 [0/320 (0%)]\tLoss: 1356.361450\n","====> Epoch: 57 Average loss: 1380.1262\n","====> Test set loss: 1415.4447\n","Train Epoch: 58 [0/320 (0%)]\tLoss: 1359.347046\n","====> Epoch: 58 Average loss: 1379.0771\n","====> Test set loss: 1414.5595\n","Train Epoch: 59 [0/320 (0%)]\tLoss: 1357.967407\n","====> Epoch: 59 Average loss: 1378.2185\n","====> Test set loss: 1415.1179\n","Train Epoch: 60 [0/320 (0%)]\tLoss: 1388.668335\n","====> Epoch: 60 Average loss: 1377.7951\n","====> Test set loss: 1414.3016\n","Train Epoch: 61 [0/320 (0%)]\tLoss: 1385.931641\n","====> Epoch: 61 Average loss: 1377.7160\n","====> Test set loss: 1414.9345\n","Train Epoch: 62 [0/320 (0%)]\tLoss: 1322.140137\n","====> Epoch: 62 Average loss: 1376.9078\n","====> Test set loss: 1414.9846\n","Train Epoch: 63 [0/320 (0%)]\tLoss: 1417.525269\n","====> Epoch: 63 Average loss: 1376.8463\n","====> Test set loss: 1416.6986\n","Train Epoch: 64 [0/320 (0%)]\tLoss: 1394.068848\n","====> Epoch: 64 Average loss: 1375.8924\n","====> Test set loss: 1415.2319\n","Train Epoch: 65 [0/320 (0%)]\tLoss: 1355.365234\n","====> Epoch: 65 Average loss: 1376.3176\n","====> Test set loss: 1416.7352\n","Train Epoch: 66 [0/320 (0%)]\tLoss: 1394.822144\n","====> Epoch: 66 Average loss: 1375.3507\n","====> Test set loss: 1415.9871\n","Train Epoch: 67 [0/320 (0%)]\tLoss: 1381.691040\n","====> Epoch: 67 Average loss: 1375.5740\n","====> Test set loss: 1415.7885\n","Train Epoch: 68 [0/320 (0%)]\tLoss: 1362.620972\n","====> Epoch: 68 Average loss: 1375.6592\n","====> Test set loss: 1415.8751\n","Train Epoch: 69 [0/320 (0%)]\tLoss: 1372.798706\n","====> Epoch: 69 Average loss: 1375.0320\n","====> Test set loss: 1416.8167\n","Train Epoch: 70 [0/320 (0%)]\tLoss: 1412.816650\n","====> Epoch: 70 Average loss: 1375.9444\n","====> Test set loss: 1421.9377\n","Train Epoch: 71 [0/320 (0%)]\tLoss: 1371.133423\n","====> Epoch: 71 Average loss: 1376.9403\n","====> Test set loss: 1418.7436\n","Train Epoch: 72 [0/320 (0%)]\tLoss: 1364.009766\n","====> Epoch: 72 Average loss: 1375.2132\n","====> Test set loss: 1417.9366\n","Train Epoch: 73 [0/320 (0%)]\tLoss: 1379.215454\n","====> Epoch: 73 Average loss: 1373.5921\n","====> Test set loss: 1417.2177\n","Train Epoch: 74 [0/320 (0%)]\tLoss: 1349.452881\n","====> Epoch: 74 Average loss: 1372.6851\n","====> Test set loss: 1417.3385\n","Train Epoch: 75 [0/320 (0%)]\tLoss: 1334.141602\n","====> Epoch: 75 Average loss: 1372.8021\n","====> Test set loss: 1418.9093\n","Train Epoch: 76 [0/320 (0%)]\tLoss: 1365.025757\n","====> Epoch: 76 Average loss: 1371.9120\n","====> Test set loss: 1418.2735\n","Train Epoch: 77 [0/320 (0%)]\tLoss: 1354.792725\n","====> Epoch: 77 Average loss: 1371.0441\n","====> Test set loss: 1418.6412\n","Train Epoch: 78 [0/320 (0%)]\tLoss: 1366.520264\n","====> Epoch: 78 Average loss: 1370.1511\n","====> Test set loss: 1418.4619\n","Train Epoch: 79 [0/320 (0%)]\tLoss: 1369.969971\n","====> Epoch: 79 Average loss: 1370.1101\n","====> Test set loss: 1419.5059\n","Train Epoch: 80 [0/320 (0%)]\tLoss: 1404.860107\n","====> Epoch: 80 Average loss: 1370.2106\n","====> Test set loss: 1418.4927\n","Train Epoch: 81 [0/320 (0%)]\tLoss: 1372.812134\n","====> Epoch: 81 Average loss: 1368.6188\n","====> Test set loss: 1419.3136\n","Train Epoch: 82 [0/320 (0%)]\tLoss: 1390.581055\n","====> Epoch: 82 Average loss: 1368.8006\n","====> Test set loss: 1420.4868\n","Train Epoch: 83 [0/320 (0%)]\tLoss: 1384.311401\n","====> Epoch: 83 Average loss: 1368.5406\n","====> Test set loss: 1419.9740\n","Train Epoch: 84 [0/320 (0%)]\tLoss: 1354.328857\n","====> Epoch: 84 Average loss: 1368.4267\n","====> Test set loss: 1421.7161\n","Train Epoch: 85 [0/320 (0%)]\tLoss: 1348.246582\n","====> Epoch: 85 Average loss: 1367.9694\n","====> Test set loss: 1422.0973\n","Train Epoch: 86 [0/320 (0%)]\tLoss: 1366.799438\n","====> Epoch: 86 Average loss: 1367.0565\n","====> Test set loss: 1420.6530\n","Train Epoch: 87 [0/320 (0%)]\tLoss: 1363.474854\n","====> Epoch: 87 Average loss: 1367.4042\n","====> Test set loss: 1423.8657\n","Train Epoch: 88 [0/320 (0%)]\tLoss: 1357.320679\n","====> Epoch: 88 Average loss: 1368.1525\n","====> Test set loss: 1425.4578\n","Train Epoch: 89 [0/320 (0%)]\tLoss: 1287.068115\n","====> Epoch: 89 Average loss: 1368.6687\n","====> Test set loss: 1423.5583\n","Train Epoch: 90 [0/320 (0%)]\tLoss: 1398.157471\n","====> Epoch: 90 Average loss: 1368.4756\n","====> Test set loss: 1420.5016\n","Train Epoch: 91 [0/320 (0%)]\tLoss: 1355.997070\n","====> Epoch: 91 Average loss: 1366.6024\n","====> Test set loss: 1421.4661\n","Train Epoch: 92 [0/320 (0%)]\tLoss: 1372.557617\n","====> Epoch: 92 Average loss: 1366.1182\n","====> Test set loss: 1424.0950\n","Train Epoch: 93 [0/320 (0%)]\tLoss: 1367.631592\n","====> Epoch: 93 Average loss: 1366.1568\n","====> Test set loss: 1423.3775\n","Train Epoch: 94 [0/320 (0%)]\tLoss: 1329.580078\n","====> Epoch: 94 Average loss: 1366.1158\n","====> Test set loss: 1422.1654\n","Train Epoch: 95 [0/320 (0%)]\tLoss: 1340.369751\n","====> Epoch: 95 Average loss: 1365.6733\n","====> Test set loss: 1424.1466\n","Train Epoch: 96 [0/320 (0%)]\tLoss: 1374.630859\n","====> Epoch: 96 Average loss: 1365.4304\n","====> Test set loss: 1425.3586\n","Train Epoch: 97 [0/320 (0%)]\tLoss: 1352.497437\n","====> Epoch: 97 Average loss: 1364.9875\n","====> Test set loss: 1422.1745\n","Train Epoch: 98 [0/320 (0%)]\tLoss: 1390.947021\n","====> Epoch: 98 Average loss: 1365.1978\n","====> Test set loss: 1422.2143\n","Train Epoch: 99 [0/320 (0%)]\tLoss: 1379.567505\n","====> Epoch: 99 Average loss: 1364.4560\n","====> Test set loss: 1423.9369\n","Train Epoch: 100 [0/320 (0%)]\tLoss: 1308.706299\n","====> Epoch: 100 Average loss: 1363.9865\n","====> Test set loss: 1423.1758\n","Train Epoch: 101 [0/320 (0%)]\tLoss: 1319.892700\n","====> Epoch: 101 Average loss: 1363.8755\n","====> Test set loss: 1422.3191\n","Train Epoch: 102 [0/320 (0%)]\tLoss: 1401.015503\n","====> Epoch: 102 Average loss: 1363.5615\n","====> Test set loss: 1423.4640\n","Train Epoch: 103 [0/320 (0%)]\tLoss: 1380.614258\n","====> Epoch: 103 Average loss: 1362.9335\n","====> Test set loss: 1425.9977\n","Train Epoch: 104 [0/320 (0%)]\tLoss: 1398.181030\n","====> Epoch: 104 Average loss: 1362.7977\n","====> Test set loss: 1425.0709\n","Train Epoch: 105 [0/320 (0%)]\tLoss: 1356.693359\n","====> Epoch: 105 Average loss: 1361.6571\n","====> Test set loss: 1423.2521\n","Train Epoch: 106 [0/320 (0%)]\tLoss: 1373.739014\n","====> Epoch: 106 Average loss: 1361.8189\n","====> Test set loss: 1427.3359\n","Train Epoch: 107 [0/320 (0%)]\tLoss: 1338.119263\n","====> Epoch: 107 Average loss: 1362.7903\n","====> Test set loss: 1424.9972\n","Train Epoch: 108 [0/320 (0%)]\tLoss: 1335.053101\n","====> Epoch: 108 Average loss: 1362.2966\n","====> Test set loss: 1424.8967\n","Train Epoch: 109 [0/320 (0%)]\tLoss: 1312.800171\n","====> Epoch: 109 Average loss: 1361.3477\n","====> Test set loss: 1429.3858\n","Train Epoch: 110 [0/320 (0%)]\tLoss: 1386.820068\n","====> Epoch: 110 Average loss: 1361.3042\n","====> Test set loss: 1425.1320\n","Train Epoch: 111 [0/320 (0%)]\tLoss: 1347.561035\n","====> Epoch: 111 Average loss: 1362.4439\n","====> Test set loss: 1425.9162\n","Train Epoch: 112 [0/320 (0%)]\tLoss: 1358.617798\n","====> Epoch: 112 Average loss: 1361.1447\n","====> Test set loss: 1428.4519\n","Train Epoch: 113 [0/320 (0%)]\tLoss: 1348.496826\n","====> Epoch: 113 Average loss: 1360.9760\n","====> Test set loss: 1426.7311\n","Train Epoch: 114 [0/320 (0%)]\tLoss: 1369.180542\n","====> Epoch: 114 Average loss: 1360.1281\n","====> Test set loss: 1422.6605\n","Train Epoch: 115 [0/320 (0%)]\tLoss: 1353.026489\n","====> Epoch: 115 Average loss: 1360.2882\n","====> Test set loss: 1427.6969\n","Train Epoch: 116 [0/320 (0%)]\tLoss: 1393.129639\n","====> Epoch: 116 Average loss: 1359.3234\n","====> Test set loss: 1428.6018\n","Train Epoch: 117 [0/320 (0%)]\tLoss: 1359.202881\n","====> Epoch: 117 Average loss: 1359.2124\n","====> Test set loss: 1426.2891\n","Train Epoch: 118 [0/320 (0%)]\tLoss: 1374.350220\n","====> Epoch: 118 Average loss: 1358.9333\n","====> Test set loss: 1428.2365\n","Train Epoch: 119 [0/320 (0%)]\tLoss: 1330.730713\n","====> Epoch: 119 Average loss: 1359.4164\n","====> Test set loss: 1426.5684\n","Train Epoch: 120 [0/320 (0%)]\tLoss: 1381.018433\n","====> Epoch: 120 Average loss: 1359.9124\n","====> Test set loss: 1428.1313\n","Train Epoch: 121 [0/320 (0%)]\tLoss: 1395.735840\n","====> Epoch: 121 Average loss: 1359.2524\n","====> Test set loss: 1424.8458\n","Train Epoch: 122 [0/320 (0%)]\tLoss: 1344.821777\n","====> Epoch: 122 Average loss: 1359.4356\n","====> Test set loss: 1435.6985\n","Train Epoch: 123 [0/320 (0%)]\tLoss: 1375.918945\n","====> Epoch: 123 Average loss: 1360.6120\n","====> Test set loss: 1427.6664\n","Train Epoch: 124 [0/320 (0%)]\tLoss: 1367.520508\n","====> Epoch: 124 Average loss: 1359.0696\n","====> Test set loss: 1424.4875\n","Train Epoch: 125 [0/320 (0%)]\tLoss: 1368.959106\n","====> Epoch: 125 Average loss: 1359.6772\n","====> Test set loss: 1431.4384\n","Train Epoch: 126 [0/320 (0%)]\tLoss: 1324.868164\n","====> Epoch: 126 Average loss: 1359.1611\n","====> Test set loss: 1428.8698\n","Train Epoch: 127 [0/320 (0%)]\tLoss: 1364.902588\n","====> Epoch: 127 Average loss: 1358.4060\n","====> Test set loss: 1426.6552\n","Train Epoch: 128 [0/320 (0%)]\tLoss: 1332.434814\n","====> Epoch: 128 Average loss: 1357.2393\n","====> Test set loss: 1427.4778\n","Train Epoch: 129 [0/320 (0%)]\tLoss: 1319.197388\n","====> Epoch: 129 Average loss: 1357.3178\n","====> Test set loss: 1427.7392\n","Train Epoch: 130 [0/320 (0%)]\tLoss: 1358.208618\n","====> Epoch: 130 Average loss: 1356.9261\n","====> Test set loss: 1427.2704\n","Train Epoch: 131 [0/320 (0%)]\tLoss: 1352.114868\n","====> Epoch: 131 Average loss: 1357.2635\n","====> Test set loss: 1431.2167\n","Train Epoch: 132 [0/320 (0%)]\tLoss: 1318.522095\n","====> Epoch: 132 Average loss: 1357.1999\n","====> Test set loss: 1430.0573\n","Train Epoch: 133 [0/320 (0%)]\tLoss: 1377.782593\n","====> Epoch: 133 Average loss: 1356.5847\n","====> Test set loss: 1427.2152\n","Train Epoch: 134 [0/320 (0%)]\tLoss: 1333.705444\n","====> Epoch: 134 Average loss: 1357.0906\n","====> Test set loss: 1432.2940\n","Train Epoch: 135 [0/320 (0%)]\tLoss: 1359.864380\n","====> Epoch: 135 Average loss: 1356.2387\n","====> Test set loss: 1429.7333\n","Train Epoch: 136 [0/320 (0%)]\tLoss: 1319.366699\n","====> Epoch: 136 Average loss: 1356.9571\n","====> Test set loss: 1426.0811\n","Train Epoch: 137 [0/320 (0%)]\tLoss: 1368.574097\n","====> Epoch: 137 Average loss: 1356.6081\n","====> Test set loss: 1430.8740\n","Train Epoch: 138 [0/320 (0%)]\tLoss: 1364.925293\n","====> Epoch: 138 Average loss: 1354.9609\n","====> Test set loss: 1427.6129\n","Train Epoch: 139 [0/320 (0%)]\tLoss: 1342.669556\n","====> Epoch: 139 Average loss: 1355.2854\n","====> Test set loss: 1429.2037\n","Train Epoch: 140 [0/320 (0%)]\tLoss: 1368.952881\n","====> Epoch: 140 Average loss: 1354.4981\n","====> Test set loss: 1429.2357\n","Train Epoch: 141 [0/320 (0%)]\tLoss: 1356.744995\n","====> Epoch: 141 Average loss: 1354.4031\n","====> Test set loss: 1432.5321\n","Train Epoch: 142 [0/320 (0%)]\tLoss: 1339.614746\n","====> Epoch: 142 Average loss: 1354.2358\n","====> Test set loss: 1428.8769\n","Train Epoch: 143 [0/320 (0%)]\tLoss: 1347.472412\n","====> Epoch: 143 Average loss: 1354.5718\n","====> Test set loss: 1430.3372\n","Train Epoch: 144 [0/320 (0%)]\tLoss: 1339.410522\n","====> Epoch: 144 Average loss: 1355.0486\n","====> Test set loss: 1433.9562\n","Train Epoch: 145 [0/320 (0%)]\tLoss: 1353.008789\n","====> Epoch: 145 Average loss: 1354.9063\n","====> Test set loss: 1429.5252\n","Train Epoch: 146 [0/320 (0%)]\tLoss: 1350.079346\n","====> Epoch: 146 Average loss: 1354.8969\n","====> Test set loss: 1432.7434\n","Train Epoch: 147 [0/320 (0%)]\tLoss: 1336.153198\n","====> Epoch: 147 Average loss: 1354.7126\n","====> Test set loss: 1431.3507\n","Train Epoch: 148 [0/320 (0%)]\tLoss: 1352.937622\n","====> Epoch: 148 Average loss: 1354.5727\n","====> Test set loss: 1431.1321\n","Train Epoch: 149 [0/320 (0%)]\tLoss: 1361.056152\n","====> Epoch: 149 Average loss: 1355.3797\n","====> Test set loss: 1428.9652\n","Train Epoch: 150 [0/320 (0%)]\tLoss: 1380.693481\n","====> Epoch: 150 Average loss: 1355.7327\n","====> Test set loss: 1434.4259\n","Train Epoch: 151 [0/320 (0%)]\tLoss: 1328.299438\n","====> Epoch: 151 Average loss: 1356.7323\n","====> Test set loss: 1435.5059\n","Train Epoch: 152 [0/320 (0%)]\tLoss: 1313.737793\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-b55ea7db3968>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4391842b177a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#重置损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#重置梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-8374b0b8bad2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# 返回每个样本的图像和注释（如果有）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2982\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2984\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for epoch in range(1, 400):\n","    train(epoch)\n","    test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PyhA1koFj2Uf"},"outputs":[],"source":["with torch.no_grad():\n","    #从标准正态分布中随机采样\n","    z = torch.randn(64, 2).to(device)\n","    #input_num=output_num,解码\n","    sample = vae.decoder(z).to(device)\n","    #save\n","    save_image(sample.view(64, 3, 28, 28), './VAE_' + '.png')"]},{"cell_type":"code","source":["#保存参数文件,自己选择保存地址\n","torch.save(vae.state_dict(), '/content/drive/MyDrive/VAE/pytorch-mnist-VAE-master/VAE_params.pkl')"],"metadata":{"id":"ryhCA_Wi8cYn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**AE**\n"],"metadata":{"id":"wphf2XiVtykx"}},{"cell_type":"code","source":["class AE(nn.Module):\n"," def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n","    super(AE, self).__init__()\n","\n","    # encoder part\n","    self.fc1 = nn.Linear(x_dim, h_dim1)\n","    self.fc2 = nn.Linear(h_dim1, h_dim2)\n","    self.fc3 = nn.Linear(h_dim2, z_dim) # only one layer for hidden variable\n","    # decoder part\n","    self.fc4 = nn.Linear(z_dim, h_dim2)\n","    self.fc5 = nn.Linear(h_dim2, h_dim1)\n","    self.fc6 = nn.Linear(h_dim1, x_dim)\n","\n"," def encoder(self, x):\n","    h = F.relu(self.fc1(x))\n","    h = F.relu(self.fc2(h))\n","    return self.fc3(h) # return hidden variable\n","\n","\n"," def decoder(self, z):\n","    h = F.relu(self.fc4(z))\n","    h = F.relu(self.fc5(h))\n","    return F.sigmoid(self.fc6(h))\n","\n"," def forward(self, x):\n","    z = self.encoder(x.view(-1, 3*784))\n","\n","\n","    return self.decoder(z), z # return reconstruction and hidden variable\n","\n","# build model\n","ae = AE(x_dim=3*784, h_dim1= 512, h_dim2=256, z_dim=2)\n","\n","ae.to(device)"],"metadata":{"id":"otzWDykuwnK3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684735867639,"user_tz":-480,"elapsed":409,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"outputId":"51f3b8f3-6f6f-4841-89f9-336ef76f96a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AE(\n","  (fc1): Linear(in_features=2352, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=256, bias=True)\n","  (fc3): Linear(in_features=256, out_features=2, bias=True)\n","  (fc4): Linear(in_features=2, out_features=256, bias=True)\n","  (fc5): Linear(in_features=256, out_features=512, bias=True)\n","  (fc6): Linear(in_features=512, out_features=2352, bias=True)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["optimizer = optim.Adam(ae.parameters())\n","# return reconstruction error\n","def loss_function(recon_x, x):\n"," BCE = F.binary_cross_entropy(recon_x, x.view(-1, 3*784), reduction='sum')\n"," return BCE # no KL divergence"],"metadata":{"id":"CdTgtT7Sw0Nj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(epoch):\n"," ae.train()\n"," train_loss = 0\n"," for batch_idx, data, in enumerate(train_loader):\n","  data = data.to(device)\n","  optimizer.zero_grad()\n","\n","  recon_batch, z = ae(data) # no mu and log_var\n","  loss = loss_function(recon_batch, data)\n","\n","  loss.backward()\n","  train_loss += loss.item()\n","  optimizer.step()\n","\n","  if batch_idx % 100 == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","      epoch, batch_idx * len(data), len(train_loader.dataset),\n","      100. * batch_idx / len(train_loader), loss.item() / len(data)))\n"," print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n","\n","def test():\n"," ae.eval()\n"," test_loss= 0\n"," with torch.no_grad():\n","  for data in test_loader:\n","    data = data.to(device)\n","    recon_batch, z = ae(data) # no mu and log_var\n","\n","    # sum up batch loss\n","    test_loss += loss_function(recon_batch, data).item()\n","\n","    test_loss /= len(test_loader.dataset)\n","  print('====> Test set loss: {:.4f}'.format(test_loss))\n","for epoch in range(1, 3000):\n","    train(epoch)\n","    test()"],"metadata":{"id":"rjxjYLXRt1_E","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1684551764458,"user_tz":-480,"elapsed":46406,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"outputId":"50f357f5-1cd3-4958-d66f-4362d39851c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/320 (0%)]\tLoss: 1631.029541\n","====> Epoch: 1 Average loss: 1617.5077\n","====> Test set loss: 332.2645\n","Train Epoch: 2 [0/320 (0%)]\tLoss: 1582.459473\n","====> Epoch: 2 Average loss: 1581.2021\n","====> Test set loss: 328.1815\n","Train Epoch: 3 [0/320 (0%)]\tLoss: 1559.066406\n","====> Epoch: 3 Average loss: 1552.6415\n","====> Test set loss: 320.7577\n","Train Epoch: 4 [0/320 (0%)]\tLoss: 1522.764160\n","====> Epoch: 4 Average loss: 1492.7379\n","====> Test set loss: 308.6075\n","Train Epoch: 5 [0/320 (0%)]\tLoss: 1430.398560\n","====> Epoch: 5 Average loss: 1459.2379\n","====> Test set loss: 304.8855\n","Train Epoch: 6 [0/320 (0%)]\tLoss: 1451.073486\n","====> Epoch: 6 Average loss: 1443.2845\n","====> Test set loss: 303.6268\n","Train Epoch: 7 [0/320 (0%)]\tLoss: 1458.935669\n","====> Epoch: 7 Average loss: 1434.5585\n","====> Test set loss: 301.5566\n","Train Epoch: 8 [0/320 (0%)]\tLoss: 1401.988892\n","====> Epoch: 8 Average loss: 1426.1610\n","====> Test set loss: 301.1121\n","Train Epoch: 9 [0/320 (0%)]\tLoss: 1405.327393\n","====> Epoch: 9 Average loss: 1422.1063\n","====> Test set loss: 300.7966\n","Train Epoch: 10 [0/320 (0%)]\tLoss: 1397.677368\n","====> Epoch: 10 Average loss: 1418.1871\n","====> Test set loss: 300.5694\n","Train Epoch: 11 [0/320 (0%)]\tLoss: 1436.237305\n","====> Epoch: 11 Average loss: 1413.4543\n","====> Test set loss: 299.5793\n","Train Epoch: 12 [0/320 (0%)]\tLoss: 1427.896484\n","====> Epoch: 12 Average loss: 1406.6066\n","====> Test set loss: 298.5708\n","Train Epoch: 13 [0/320 (0%)]\tLoss: 1406.510742\n","====> Epoch: 13 Average loss: 1396.6222\n","====> Test set loss: 297.2416\n","Train Epoch: 14 [0/320 (0%)]\tLoss: 1404.293701\n","====> Epoch: 14 Average loss: 1390.5991\n","====> Test set loss: 296.6579\n","Train Epoch: 15 [0/320 (0%)]\tLoss: 1387.664062\n","====> Epoch: 15 Average loss: 1389.4207\n","====> Test set loss: 296.8004\n","Train Epoch: 16 [0/320 (0%)]\tLoss: 1408.235107\n","====> Epoch: 16 Average loss: 1387.9475\n","====> Test set loss: 296.9349\n","Train Epoch: 17 [0/320 (0%)]\tLoss: 1354.258789\n","====> Epoch: 17 Average loss: 1385.8932\n","====> Test set loss: 296.3475\n","Train Epoch: 18 [0/320 (0%)]\tLoss: 1393.323120\n","====> Epoch: 18 Average loss: 1384.6365\n","====> Test set loss: 296.2424\n","Train Epoch: 19 [0/320 (0%)]\tLoss: 1383.618408\n","====> Epoch: 19 Average loss: 1383.9877\n","====> Test set loss: 296.5025\n","Train Epoch: 20 [0/320 (0%)]\tLoss: 1377.025391\n","====> Epoch: 20 Average loss: 1382.5021\n","====> Test set loss: 295.9608\n","Train Epoch: 21 [0/320 (0%)]\tLoss: 1383.453491\n","====> Epoch: 21 Average loss: 1381.3566\n","====> Test set loss: 295.9415\n","Train Epoch: 22 [0/320 (0%)]\tLoss: 1394.359253\n","====> Epoch: 22 Average loss: 1380.5492\n","====> Test set loss: 296.1020\n","Train Epoch: 23 [0/320 (0%)]\tLoss: 1380.136719\n","====> Epoch: 23 Average loss: 1380.2209\n","====> Test set loss: 295.7633\n","Train Epoch: 24 [0/320 (0%)]\tLoss: 1378.482788\n","====> Epoch: 24 Average loss: 1379.7834\n","====> Test set loss: 295.9715\n","Train Epoch: 25 [0/320 (0%)]\tLoss: 1378.609131\n","====> Epoch: 25 Average loss: 1379.5419\n","====> Test set loss: 295.8849\n","Train Epoch: 26 [0/320 (0%)]\tLoss: 1356.556396\n","====> Epoch: 26 Average loss: 1379.5387\n","====> Test set loss: 295.9849\n","Train Epoch: 27 [0/320 (0%)]\tLoss: 1375.877808\n","====> Epoch: 27 Average loss: 1378.8347\n","====> Test set loss: 295.8699\n","Train Epoch: 28 [0/320 (0%)]\tLoss: 1377.157837\n","====> Epoch: 28 Average loss: 1378.1597\n","====> Test set loss: 296.1187\n","Train Epoch: 29 [0/320 (0%)]\tLoss: 1358.571045\n","====> Epoch: 29 Average loss: 1377.9963\n","====> Test set loss: 296.0273\n","Train Epoch: 30 [0/320 (0%)]\tLoss: 1385.282715\n","====> Epoch: 30 Average loss: 1377.6202\n","====> Test set loss: 295.8560\n","Train Epoch: 31 [0/320 (0%)]\tLoss: 1413.146118\n","====> Epoch: 31 Average loss: 1377.5667\n","====> Test set loss: 296.0178\n","Train Epoch: 32 [0/320 (0%)]\tLoss: 1425.494385\n","====> Epoch: 32 Average loss: 1377.3144\n","====> Test set loss: 296.0319\n","Train Epoch: 33 [0/320 (0%)]\tLoss: 1389.252686\n","====> Epoch: 33 Average loss: 1376.5083\n","====> Test set loss: 295.8467\n","Train Epoch: 34 [0/320 (0%)]\tLoss: 1378.084961\n","====> Epoch: 34 Average loss: 1376.2309\n","====> Test set loss: 295.9654\n","Train Epoch: 35 [0/320 (0%)]\tLoss: 1406.853027\n","====> Epoch: 35 Average loss: 1375.8601\n","====> Test set loss: 295.6810\n","Train Epoch: 36 [0/320 (0%)]\tLoss: 1403.641602\n","====> Epoch: 36 Average loss: 1375.4893\n","====> Test set loss: 296.1785\n","Train Epoch: 37 [0/320 (0%)]\tLoss: 1398.553101\n","====> Epoch: 37 Average loss: 1374.9457\n","====> Test set loss: 295.6820\n","Train Epoch: 38 [0/320 (0%)]\tLoss: 1369.363770\n","====> Epoch: 38 Average loss: 1374.5714\n","====> Test set loss: 295.7832\n","Train Epoch: 39 [0/320 (0%)]\tLoss: 1405.805420\n","====> Epoch: 39 Average loss: 1374.2728\n","====> Test set loss: 296.3528\n","Train Epoch: 40 [0/320 (0%)]\tLoss: 1359.468994\n","====> Epoch: 40 Average loss: 1373.6508\n","====> Test set loss: 295.7380\n","Train Epoch: 41 [0/320 (0%)]\tLoss: 1392.878052\n","====> Epoch: 41 Average loss: 1373.3927\n","====> Test set loss: 295.7746\n","Train Epoch: 42 [0/320 (0%)]\tLoss: 1350.207764\n","====> Epoch: 42 Average loss: 1372.3771\n","====> Test set loss: 295.9041\n","Train Epoch: 43 [0/320 (0%)]\tLoss: 1329.141357\n","====> Epoch: 43 Average loss: 1372.1136\n","====> Test set loss: 295.9571\n","Train Epoch: 44 [0/320 (0%)]\tLoss: 1378.328979\n","====> Epoch: 44 Average loss: 1371.3690\n","====> Test set loss: 295.6598\n","Train Epoch: 45 [0/320 (0%)]\tLoss: 1388.305542\n","====> Epoch: 45 Average loss: 1370.7455\n","====> Test set loss: 295.9183\n","Train Epoch: 46 [0/320 (0%)]\tLoss: 1362.570557\n","====> Epoch: 46 Average loss: 1370.3415\n","====> Test set loss: 296.1867\n","Train Epoch: 47 [0/320 (0%)]\tLoss: 1358.921509\n","====> Epoch: 47 Average loss: 1369.5510\n","====> Test set loss: 295.9700\n","Train Epoch: 48 [0/320 (0%)]\tLoss: 1388.938843\n","====> Epoch: 48 Average loss: 1369.0498\n","====> Test set loss: 295.8553\n","Train Epoch: 49 [0/320 (0%)]\tLoss: 1367.727295\n","====> Epoch: 49 Average loss: 1368.5563\n","====> Test set loss: 295.9902\n","Train Epoch: 50 [0/320 (0%)]\tLoss: 1385.672485\n","====> Epoch: 50 Average loss: 1368.2428\n","====> Test set loss: 296.2617\n","Train Epoch: 51 [0/320 (0%)]\tLoss: 1365.636719\n","====> Epoch: 51 Average loss: 1367.6935\n","====> Test set loss: 296.0466\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-92c333459348>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'====> Test set loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-92c333459348>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m  \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m  \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-8374b0b8bad2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# 返回每个样本的图像和注释（如果有）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["#保存重构图片\n","AE and VAE 都跑大约3K次进行比对"],"metadata":{"id":"3bxdFy6svHoJ"}},{"cell_type":"code","source":["subset = torch.utils.data.Subset(test_dataset, indices=torch.randperm(len(test_dataset))[:64])\n","loader = torch.utils.data.DataLoader(subset, batch_size=64, shuffle=False)\n","with torch.no_grad():\n"," for data in loader:\n","  data=data.to(device)\n","  recon_batch, z = ae(data)\n","  save_image(data.view(64, 3, 28, 28), './orig_' + '.png')\n","  save_image(recon_batch.view(64, 3, 28, 28), './AE_recon_' + '.png')\n","\n","  recon_batch, _, _ = vae(data)\n","\n","  save_image(recon_batch.view(64, 3, 28, 28), './VAE_recon_' + '.png')"],"metadata":{"id":"YGYh2roAvnTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(ae.state_dict(), '/content/drive/MyDrive/VAE/pytorch-mnist-VAE-master/AE_params.pkl')"],"metadata":{"id":"kNfHIpYp4o8g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["只用MSE评估"],"metadata":{"id":"HpmiVrSE5zgp"}},{"cell_type":"code","source":["#进行量化评估\n","#MSE\n","def evaluate_reconstruction(model, loader):\n","    model.eval()\n","    mse_loss = nn.MSELoss(reduction='mean')\n","    total_loss = 0.0\n","    with torch.no_grad():\n","        for images in loader:\n","            images = images.view(-1, 3*784)\n","            images = images.to(device)\n","            reconstructions= model(images)[0]\n","            loss = mse_loss(reconstructions, images)\n","            total_loss += loss.item() * images.size(0)\n","\n","    # 计算平均重建损失\n","    avg_loss = total_loss / len(loader.dataset)\n","    return avg_loss\n","\n","ae_reconstruction_loss = evaluate_reconstruction(ae, test_loader)\n","vae_reconstruction_loss = evaluate_reconstruction(vae, test_loader)\n","\n","print(f'AE Reconstruction Loss: {ae_reconstruction_loss:.4f}')\n","print(f'VAE Reconstruction Loss: {vae_reconstruction_loss:.4f}')"],"metadata":{"id":"28MQsI758roZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684481024505,"user_tz":-480,"elapsed":3,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"outputId":"5663d132-e04e-473b-80e3-496a5e609b0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AE Reconstruction Loss: 0.0639\n","VAE Reconstruction Loss: 0.0664\n"]}]},{"cell_type":"markdown","source":["使用MSE和峰值信噪比作为评估指标"],"metadata":{"id":"f581Oj5H6OSs"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","def calculate_psnr(reconstructions, originals, max_pixel_value=1.0):\n","    mse = F.mse_loss(reconstructions, originals)\n","    psnr = 20 * torch.log10(torch.tensor(max_pixel_value)) - 10 * torch.log10(mse)\n","    return psnr\n","\n","def evaluate_reconstruction(model, loader):\n","    model.eval()\n","    mse_loss = nn.MSELoss(reduction='mean')\n","    total_loss = 0.0\n","    total_psnr = 0.0\n","    with torch.no_grad():\n","        for images in loader:\n","            images = images.view(-1, 3*784)\n","            images = images.to(device)\n","            reconstructions = model(images)[0]\n","            loss = mse_loss(reconstructions, images)\n","            total_loss += loss.item() * images.size(0)\n","            psnr = calculate_psnr(reconstructions, images)\n","            total_psnr += psnr.item() * images.size(0)\n","\n","    avg_loss = total_loss / len(loader.dataset)\n","    avg_psnr = total_psnr / len(loader.dataset)\n","    return avg_loss, avg_psnr\n","A\n","# 加载AE模型参数\n","ae = AE(x_dim=3*784, h_dim1= 512, h_dim2=256, z_dim=2)\n","ae.load_state_dict(torch.load('/content/drive/MyDrive/VAE/pytorch-mnist-VAE-master/pytorch_celeba_VAE/AE_params.pkl', map_location=torch.device('cpu')))\n","ae.to(device)\n","\n","# 加载VAE模型参数\n","vae = VAE(x_dim=3*784, h_dim1= 512, h_dim2=256, z_dim=2)\n","vae.load_state_dict(torch.load('/content/drive/MyDrive/VAE/pytorch-mnist-VAE-master/pytorch_celeba_VAE/VAE_params.pkl', map_location=torch.device('cpu')))\n","vae.to(device)\n","\n","ae_reconstruction_loss, ae_psnr = evaluate_reconstruction(ae, test_loader)\n","vae_reconstruction_loss, vae_psnr = evaluate_reconstruction(vae, test_loader)\n","\n","print(f'AE Reconstruction Loss: {ae_reconstruction_loss:.4f}')\n","print(f'AE PSNR: {ae_psnr:.2f} dB')\n","print(f'VAE Reconstruction Loss: {vae_reconstruction_loss:.4f}')\n","print(f'VAE PSNR: {vae_psnr:.2f} dB')\n"],"metadata":{"id":"Cx8Z0sFJquVH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684735932629,"user_tz":-480,"elapsed":1336,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"outputId":"e60727ee-acbf-42f5-e007-4f6cdde9cfac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AE Reconstruction Loss: 0.0194\n","AE PSNR: 17.13 dB\n","VAE Reconstruction Loss: 0.0203\n","VAE PSNR: 16.96 dB\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"id":"hSGUaMEQrdn0","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1684735194236,"user_tz":-480,"elapsed":7,"user":{"displayName":"YL D","userId":"06733600836656624252"}},"outputId":"c8407fea-d7f7-461e-da67-9e0d3e7c435d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["num_steps = 10  # 插值步数\n","with torch.no_grad():\n","    # 从潜在空间中采样两个点\n","    z1 = torch.randn(1, 2).to(device)  # 第一个点\n","    z2 = torch.randn(1, 2).to(device)  # 第二个点\n","\n","    # 生成插值的图像\n","    interpolated_images = []\n","    for step in range(num_steps):\n","        # 在两个潜在点之间进行线性插值\n","        alpha = float(step) / (num_steps - 1)\n","        interpolated_z = (1 - alpha) * z1 + alpha * z2\n","\n","        # 解码插值的潜在点以生成图像\n","        interpolated_image = vae.decoder(interpolated_z)\n","\n","        interpolated_images.append(interpolated_image)\n","\n","    # 拼接并保存插值的图像\n","    interpolated_images = torch.cat(interpolated_images, dim=0)\n","    save_image(interpolated_images.view(num_steps, 3, 28, 28), './interpolated_images.png')"],"metadata":{"id":"HS9qXa-sCuoO"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}